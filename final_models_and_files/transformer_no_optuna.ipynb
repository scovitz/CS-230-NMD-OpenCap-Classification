{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ce73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Dropout, Input, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, classification_report, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # May improve speed on some CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57464793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # May improve speed on some CPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc89001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trc(fpath):\n",
    "    # read metadata in file header\n",
    "    df_meta = pd.read_csv(fpath, delimiter='\\t', header=0, skiprows=1, nrows=1)\n",
    "    meta = df_meta.iloc[0].to_dict()\n",
    "    fps = meta['DataRate']\n",
    "\n",
    "    # read marker location names\n",
    "    markers_df = pd.read_csv(fpath, delimiter='\\t', header=None, skiprows=3, nrows=1)\n",
    "    markers = markers_df.iloc[0].dropna().to_numpy()[2:]\n",
    "\n",
    "    # read marker XYZ locations\n",
    "    df = pd.read_csv(fpath, delimiter='\\t', header=0, skiprows=4)\n",
    "    df.rename(columns=dict(zip(df.columns[:2], ('n', 't'))), inplace=True)\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "    N = df.shape[0]\n",
    "    M = len(markers)\n",
    "    xyz = df.iloc[:,2:].to_numpy().reshape((N, M, 3))\n",
    "    xyz[:,:,[0,1,2]] = xyz[:,:,[2,1,0]]\n",
    "\n",
    "    return fps, markers, xyz\n",
    "\n",
    "\n",
    "def read_mot(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        # count = 0\n",
    "        line = f.readline().strip()\n",
    "        # while line and line.strip() != 'endheader':\n",
    "        while line.lower() != 'endheader':\n",
    "            line = f.readline().strip()\n",
    "            # count += 1\n",
    "\n",
    "        # df = pd.read_csv(f, delimiter='\\t', header=0, skiprows=count-3)\n",
    "        df = pd.read_csv(f, delimiter='\\t', header=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ce729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(data_list):\n",
    "    \"\"\"\n",
    "    Applies z-score normalization to a list of time series arrays.\n",
    "    Arguments:\n",
    "        data_list: List of numpy arrays, each of shape (num_timesteps, num_features).\n",
    "    Returns:\n",
    "        normalized_data: List of z-score normalized arrays.\n",
    "        mean: Mean of each feature across all time series.\n",
    "        std: Standard deviation of each feature across all time series.\n",
    "    \"\"\"\n",
    "    # Concatenate all data to compute global mean and std\n",
    "    all_data = np.concatenate(data_list, axis=0)\n",
    "    mean = np.mean(all_data, axis=0)\n",
    "    std = np.std(all_data, axis=0)\n",
    "    \n",
    "    # Normalize each array\n",
    "    normalized_data = [(data - mean) / (std + 1e-8) for data in data_list]\n",
    "    return normalized_data, mean, std\n",
    "\n",
    "def prepare_lstm_input(subject_dict):\n",
    "    \"\"\"\n",
    "    Combines multiple time series for each subject along the time axis, applies z-score normalization,\n",
    "    and pads them for LSTM input.\n",
    "    Arguments:\n",
    "        subject_dict: Dictionary where keys are subject IDs and values are lists of 6 numpy arrays.\n",
    "                      Each numpy array corresponds to a time series with shape (num_timesteps, num_features).\n",
    "    Returns:\n",
    "        padded_input: A numpy array of shape (num_subjects, max_timesteps, num_features).\n",
    "        subject_ids: List of subject IDs in the order they appear in the padded_input array.\n",
    "        mean: Mean of features used for z-score normalization.\n",
    "        std: Standard deviation of features used for z-score normalization.\n",
    "    \"\"\"\n",
    "    # Sort the dictionary by keys to ensure consistent ordering\n",
    "    subject_ids = sorted(subject_dict.keys())\n",
    "    \n",
    "    # Extract and concatenate the 6 time series for each subject along the time axis\n",
    "    concatenated_series = [\n",
    "        np.concatenate(subject_dict[subject_id], axis=0) for subject_id in subject_ids\n",
    "    ]\n",
    "    \n",
    "    # Apply z-score normalization to the concatenated series\n",
    "    normalized_series, mean, std = z_score_normalize(concatenated_series)\n",
    "    \n",
    "    # Determine the maximum length for padding\n",
    "    max_length = max(series.shape[0] for series in normalized_series)\n",
    "    \n",
    "    # Pad all normalized time series to the same length\n",
    "    padded_input = pad_sequences(normalized_series, maxlen=max_length, padding=\"post\", dtype=\"float32\")\n",
    "    \n",
    "    return padded_input, subject_ids, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186210db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_for_participant_trial(class_df, pid, trial):\n",
    "    \"\"\"\n",
    "    Fetches the label for a given participant and trial based on class_info.csv.\n",
    "    \n",
    "    Parameters:\n",
    "        participant_id (str): The ID of the participant.\n",
    "        trial (str): The trial name or ID.\n",
    "    \n",
    "    Returns:\n",
    "        int: Classification label (0 or 1).\n",
    "    \"\"\"\n",
    "    label = class_df.loc[class_df['ID'] == pid, 'Class'].values[0]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74739701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing .mot files\n",
    "root_dir = '/Users/scovitz/datadir/opencap_data'  # path to root data dir \n",
    "labels_csv = '/Users/scovitz/datadir/class_info.csv'\n",
    "class_df = pd.read_csv(labels_csv)\n",
    "\n",
    "time_series_data = {}  # dictionary of pid, timeseries pairs \n",
    "\n",
    "expected_columns = [\n",
    "        'pelvis_tilt', 'pelvis_list', 'pelvis_rotation', 'pelvis_tx',\n",
    "        'pelvis_ty', 'pelvis_tz', 'hip_flexion_r', 'hip_adduction_r',\n",
    "        'hip_rotation_r', 'knee_angle_r', 'ankle_angle_r',\n",
    "        'subtalar_angle_r', 'mtp_angle_r', 'hip_flexion_l', 'hip_adduction_l',\n",
    "        'hip_rotation_l', 'knee_angle_l', 'ankle_angle_l',\n",
    "        'subtalar_angle_l', 'mtp_angle_l', 'lumbar_extension', 'lumbar_bending',\n",
    "        'lumbar_rotation', 'arm_flex_r', 'arm_add_r', 'arm_rot_r',\n",
    "        'elbow_flex_r', 'pro_sup_r', 'arm_flex_l', 'arm_add_l', 'arm_rot_l',\n",
    "        'elbow_flex_l', 'pro_sup_l'\n",
    "    ]\n",
    "\n",
    "# Loop through all .mot files\n",
    "for i in range(129):\n",
    "        # ensure going in order!\n",
    "        num = str(i+1).zfill(3)\n",
    "\n",
    "        for path in Path(root_dir).rglob(f\"P{num}*/**/*.mot\"): \n",
    "            \n",
    "            # remove upper extremity trials \n",
    "            if \"brooke\" in str(path) or \"curls\" in str(path) or \"arm_rom\" in str(path):\n",
    "                continue \n",
    "            \n",
    "            trial = str(path).split('/')[-3]\n",
    "            pid = str(path).split('/')[-4]\n",
    "            activity = str(path).split('/')[-2]\n",
    "            #print(str(path))\n",
    "            # Load the .mot file\n",
    "            \n",
    "            data = read_mot(str(path))\n",
    "            data = data[200:]\n",
    "            #print(data.shape)\n",
    "\n",
    "            # Ensure the file has the expected columns\n",
    "            if not all(col in data.columns for col in expected_columns + ['time']):\n",
    "                raise ValueError(f\"File {path} does not contain the expected columns.\")\n",
    "            \n",
    "            # Drop the 'time' column and keep only relevant features\n",
    "            data = np.array(data[expected_columns])\n",
    "            \n",
    "            if pid not in time_series_data:\n",
    "                time_series_data[pid + '_' + trial] = []\n",
    "\n",
    "            time_series_data[(pid + '_' + trial)].append(data) # value = list of timeseries arrays of num_timesteps x num_feats\n",
    "            print(data.shape, pid, trial, str(path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58503d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(time_series_data)\n",
    "time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the LSTM input\n",
    "lstm_input, subject_ids, mean, std = prepare_lstm_input(time_series_data)\n",
    "\n",
    "# Check the results\n",
    "print(\"LSTM Input Shape:\", lstm_input.shape)  # (162, max_timesteps, 35)\n",
    "print(\"Subject IDs:\", subject_ids[:5])  # subject ID ordering, should be in order! \n",
    "print(\"Mean Shape:\", mean.shape)  # shape of mean: (35,)\n",
    "print(\"Std Shape:\", std.shape)  # shape of std: (35,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34837cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to a numpy array\n",
    "labels = []\n",
    "for sid in subject_ids:\n",
    "    pid = sid.split('_')[0]\n",
    "    trial = sid.split('_')[1]\n",
    "    label = get_label_for_participant_trial(class_df, pid, trial)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073af037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac282738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "def data_generator(X, y, batch_size):\n",
    "    num_samples = len(X)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            X_batch = X[offset:offset+batch_size]\n",
    "            y_batch = y[offset:offset+batch_size]\n",
    "            yield np.array(X_batch), np.array(y_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155da38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets - 80, 10, 10\n",
    "def split_data(X, y, test_size=0.1, val_size=0.1):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets.\n",
    "    Arguments:\n",
    "        X: Input features (numpy array).\n",
    "        y: Labels (numpy array).\n",
    "        test_size: Proportion of data for the test set.\n",
    "        val_size: Proportion of data for the validation set from the training set.\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test3\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(lstm_input, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf98e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTIONS\n",
    "# - scope of project/priorities? one model? multiple? for 3 ppl?\n",
    "# - AWS GPU credits? need? way to do local or sherlock? \n",
    "# - can we do 80/20 split with data we have train/test? do we need dev? \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f075985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVICE\n",
    "# spend a lot of time on analysis portion - why did this occur if we were moving forward what would we change \n",
    "# try predicting TFT time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRYING OPTUNA FOR HYPERPARAM OPTIMIZATION #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, Input, GlobalAveragePooling1D\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, regularizer=None):\n",
    "#         super(MultiHeadSelfAttention, self).__init__()\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.num_heads = num_heads\n",
    "#         self.projection_dim = embed_dim // num_heads\n",
    "#         self.query_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "#         self.key_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "#         self.value_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "#         self.combine_heads = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "\n",
    "#     def attention(self, query, key, value):\n",
    "#         score = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(self.projection_dim, query.dtype))\n",
    "#         weights = tf.nn.softmax(score, axis=-1)\n",
    "#         return tf.matmul(weights, value), weights\n",
    "\n",
    "#     def split_heads(self, x, batch_size):\n",
    "#         x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "#         return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "#         batch_size = tf.shape(inputs)[0]\n",
    "#         query = self.query_dense(inputs)\n",
    "#         key = self.key_dense(inputs)\n",
    "#         value = self.value_dense(inputs)\n",
    "\n",
    "#         query = self.split_heads(query, batch_size)\n",
    "#         key = self.split_heads(key, batch_size)\n",
    "#         value = self.split_heads(value, batch_size)\n",
    "\n",
    "#         attention, _ = self.attention(query, key, value)\n",
    "#         attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "#         concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "#         return self.combine_heads(concat_attention)\n",
    "\n",
    "# class TransformerBlock(tf.keras.layers.Layer):\n",
    "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, regularizer=None):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "#         self.att = MultiHeadSelfAttention(embed_dim, num_heads, regularizer)\n",
    "#         self.ffn = tf.keras.Sequential([\n",
    "#             Dense(ff_dim, activation='relu', kernel_regularizer=regularizer),\n",
    "#             Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "#         ])\n",
    "#         self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "#         self.dropout1 = Dropout(rate)\n",
    "#         self.dropout2 = Dropout(rate)\n",
    "\n",
    "#     def call(self, inputs, training):\n",
    "#         attn_output = self.att(inputs)\n",
    "#         attn_output = self.dropout1(attn_output, training=training)\n",
    "#         out1 = self.layernorm1(inputs + attn_output)\n",
    "#         ffn_output = self.ffn(out1)\n",
    "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
    "#         return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import optuna\n",
    "# import tensorflow as tf\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.metrics import f1_score\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Disable GPU for CPU-only machine\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# # Compute class weights\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "# class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# # Define the model structure\n",
    "# def build_model(trial, input_shape):\n",
    "#     # Suggest hyperparameters\n",
    "#     embed_dim = trial.suggest_int(\"embed_dim\", 32, 128, step=32)  # Embedding dimension\n",
    "#     num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)      # Number of attention heads\n",
    "#     ff_dim = trial.suggest_int(\"ff_dim\", 64, 256, step=64)        # Feedforward network dimension\n",
    "#     num_blocks = trial.suggest_int(\"num_blocks\", 1, 3)            # Number of Transformer blocks\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "#     l2_reg = trial.suggest_float(\"l2_reg\", 1e-4, 1e-2, log=True)  # L2 regularization strength\n",
    "\n",
    "#     # Input layer\n",
    "#     inputs = tf.keras.Input(shape=input_shape)\n",
    "#     x = tf.keras.layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(inputs)\n",
    "    \n",
    "#     # Add Transformer blocks\n",
    "#     for _ in range(num_blocks):\n",
    "#         x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate, regularizer=tf.keras.regularizers.l2(l2_reg))(x, training=True)\n",
    "    \n",
    "#     # Global pooling and output layer\n",
    "#     x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "#     x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "#     outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
    "\n",
    "#     # Compile the model\n",
    "#     learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#                   loss=\"binary_crossentropy\",\n",
    "#                   metrics=[\"accuracy\"])\n",
    "#     return model\n",
    "\n",
    "# # Define the objective function\n",
    "# def objective(trial):\n",
    "#     epochs = trial.suggest_int(\"epochs\", 50, 200, step=50)  # Suggest number of epochs\n",
    "#     model = build_model(trial, input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "#     # Train the model with class weights\n",
    "#     history = model.fit(\n",
    "#         X_train, y_train,\n",
    "#         validation_data=(X_val, y_val),\n",
    "#         epochs=epochs,\n",
    "#         batch_size=16,\n",
    "#         class_weight=class_weight_dict,\n",
    "#         verbose=0  # Suppress output\n",
    "#     )\n",
    "    \n",
    "#     # Return the validation loss (minimized by Optuna)\n",
    "#     return min(history.history['val_loss'])\n",
    "\n",
    "# # Create and run the Optuna study\n",
    "# study = optuna.create_study(direction=\"minimize\")  # Minimize validation loss\n",
    "# study.optimize(objective, n_trials=5)  # Adjust n_trials as needed\n",
    "\n",
    "# # Display the best parameters\n",
    "# print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "# # Build and train the final model with the best parameters\n",
    "# best_params = study.best_params\n",
    "# final_model = build_model(\n",
    "#     trial=optuna.trial.FixedTrial(best_params),  # Use fixed parameters\n",
    "#     input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "# )\n",
    "# history = final_model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=best_params['epochs'],  # Use optimal number of epochs\n",
    "#     batch_size=16,\n",
    "#     class_weight=class_weight_dict,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the training, validation, and test sets\n",
    "# train_loss, train_accuracy = final_model.evaluate(X_train, y_train, verbose=0)\n",
    "# val_loss, val_accuracy = final_model.evaluate(X_val, y_val, verbose=0)\n",
    "# test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# # Predict to calculate F1 scores\n",
    "# y_train_pred = (final_model.predict(X_train) > 0.5).astype(int)\n",
    "# y_val_pred = (final_model.predict(X_val) > 0.5).astype(int)\n",
    "# y_test_pred = (final_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "# val_f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "# test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "# # Print the final metrics\n",
    "# print(f\"Final Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Weighted F1 Score: {train_f1:.4f}\")\n",
    "# print(f\"Final Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Weighted F1 Score: {val_f1:.4f}\")\n",
    "# print(f\"Final Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Weighted F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# # Plot the training and validation loss over all epochs\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "# plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Plot the training and validation F1 scores over all epochs\n",
    "# # Add F1 scores to the history for tracking (retraining is required for this)\n",
    "# # Assume a method to log F1 scores during training in a custom callback if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb59cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the Optuna study\n",
    "# study = optuna.create_study(direction=\"minimize\")  # Minimize 1 - weighted F1 score\n",
    "# study.optimize(objective, n_trials=5)  # Reduce n_trials for testing\n",
    "\n",
    "# # Display the best parameters\n",
    "# print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e59d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the final model with the best hyperparameters\n",
    "# #best_params = study.best_params\n",
    "\n",
    "## BEST PARAMS FOR F1 SCORE\n",
    "# best_params = {'epochs': 130, 'embed_dim': 128, 'num_heads': 8, 'ff_dim': 256, 'num_blocks': 1, 'dropout_rate': 0.35062671547806423, 'l2_reg': 0.00010345073760147071, 'learning_rate': 0.00024969942836710864}\n",
    "\n",
    "\n",
    "# # Build the final model with fixed parameters\n",
    "# final_model = build_model(\n",
    "#     trial=optuna.trial.FixedTrial(best_params),  # Use fixed trial with best params\n",
    "#     input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "# )\n",
    "\n",
    "# history = final_model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=best_params['epochs'],  # Use optimal number of epochs\n",
    "#     batch_size=16,\n",
    "#     class_weight=class_weight_dict,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on the training, validation, and test sets\n",
    "# train_loss, train_accuracy = final_model.evaluate(X_train, y_train, verbose=0)\n",
    "# val_loss, val_accuracy = final_model.evaluate(X_val, y_val, verbose=0)\n",
    "# test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# # Predict to calculate F1 scores\n",
    "# y_train_pred = (final_model.predict(X_train) > 0.5).astype(int)\n",
    "# y_val_pred = (final_model.predict(X_val) > 0.5).astype(int)\n",
    "# y_test_pred = (final_model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "# train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\")\n",
    "# val_f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "# test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "# # Print the final metrics\n",
    "# print(f\"Final Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Weighted F1 Score: {train_f1:.4f}\")\n",
    "# print(f\"Final Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Weighted F1 Score: {val_f1:.4f}\")\n",
    "# print(f\"Final Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}, Weighted F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "# # Plot the training and validation loss over all epochs\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "# plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training and Validation Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### TRANSFORMER ARCHITECTURE!!!!! \n",
    "\n",
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, regularizer=None):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.key_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.value_dense = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        self.combine_heads = Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.sqrt(tf.cast(self.projection_dim, query.dtype))\n",
    "        weights = tf.nn.softmax(score, axis=-1)\n",
    "        return tf.matmul(weights, value), weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        return self.combine_heads(concat_attention)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, regularizer=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads, regularizer)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu', kernel_regularizer=regularizer),\n",
    "            Dense(embed_dim, kernel_regularizer=regularizer)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def build_transformer_model(input_shape, num_heads=4, embed_dim=64, ff_dim=128, num_blocks=2, l2_reg=1e-3):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Embedding layer with regularization\n",
    "    x = Dense(embed_dim, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    \n",
    "    # Add Transformer blocks with regularization\n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=0.3, regularizer=l2(l2_reg))(x, training=True)\n",
    "\n",
    "    # Global pooling and output layer\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.5)(x) # INCREASED FROM 0.3\n",
    "    outputs = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(l2_reg))(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    # learning rate decreaesd from 0.001\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build the Transformer model\n",
    "input_shape = (lstm_input.shape[1], lstm_input.shape[2])  # timesteps, features\n",
    "transformer_model = build_transformer_model(input_shape)\n",
    "\n",
    "# Summary of the model\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d119c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Transformer model\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "## MAYBE TRY BRINGING BACK EARLY STOPPING BUT WITH A DIFFERENT PATIENCE \n",
    "## MAYBE TRY CLASS WEIGHTS \n",
    "\n",
    "# Compute class weights to balance the dataset\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# lr_scheduler = ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.5,  # Reduce the learning rate by half\n",
    "#     patience=5,  # Wait for 5 epochs without improvement\n",
    "#     min_lr=1e-6  # Minimum learning rate\n",
    "# )\n",
    "\n",
    "history = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=70,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping], # Trying bringing back early stopping or lr_scheduler\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Print final training and validation accuracy\n",
    "print(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = transformer_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = (transformer_model.predict(X_test) > 0.5).astype(\"int32\")  # Threshold at 0.5\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "test_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"Weighted F1 Score: \", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs - Kinematic Timeseries')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final training and validation accuracy\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "print(f\"Final Training Accuracy: {train_accuracy[-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracy[-1]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = transformer_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_prob = transformer_model.predict(X_test).ravel()  # Predicted probabilities\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Threshold at 0.5\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "weighted_recall = classification_report(y_test, (y_pred_prob > 0.5).astype(int), output_dict=True)['weighted avg']['recall']\n",
    "weighted_precision = classification_report(y_test, (y_pred_prob > 0.5).astype(int), output_dict=True)['weighted avg']['precision']\n",
    "\n",
    "print('weighted average precision: ', weighted_precision)\n",
    "print(\"average recall: \", weighted_recall)\n",
    "\n",
    "\n",
    "# Compute and print Weighted F1 Score\n",
    "test_f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(\"Weighted F1 Score: \", test_f1)\n",
    "\n",
    "# ROC-AUC and Precision-Recall Curves\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "average_precision = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})', color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f'PR curve (AUPRC = {average_precision:.2f})', color='green')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall (PR) Curve')\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print ROC-AUC and PR-AUC scores\n",
    "print(f\"\\nFinal ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"Final Precision-Recall AUC (AUPRC): {average_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15696a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Assuming `transformer_model` is the built model from your code\n",
    "# Save the architecture diagram to a file\n",
    "plot_model(transformer_model, to_file='transformer_model_architecture.png', \n",
    "           show_shapes=True, show_layer_names=True, expand_nested=True, dpi=96)\n",
    "\n",
    "# Display the diagram inline (requires the file saved above)\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and display the saved image\n",
    "img = Image.open('transformer_model_architecture.png')\n",
    "plt.figure(figsize=(10, 15))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axes for better visualization\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817bb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Trying to Find the Ideal Hyperparameters Using Grid Search\n",
    "\n",
    "\n",
    "# # Define the model-building function\n",
    "# def build_model(embed_dim, num_heads, ff_dim, num_blocks, l2_reg, dropout_rate):\n",
    "#     inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "#     x = Dense(embed_dim, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "#     for _ in range(num_blocks):\n",
    "#         x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate, regularizer=l2(l2_reg))(x, training=True)\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     outputs = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(l2_reg))(x)\n",
    "\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Wrap the Keras model for GridSearchCV\n",
    "# model = KerasClassifier(\n",
    "#     model=build_model,\n",
    "#     embed_dim=64,  # Default value, will be overridden by GridSearch\n",
    "#     num_heads=4,\n",
    "#     ff_dim=128,\n",
    "#     num_blocks=2,\n",
    "#     l2_reg=1e-3,\n",
    "#     dropout_rate=0.3,\n",
    "#     epochs=50,  # Use fewer epochs for tuning\n",
    "#     batch_size=16,\n",
    "#     verbose=0\n",
    "# )\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'embed_dim': [32, 64, 128],\n",
    "#     'num_heads': [2, 4, 6],\n",
    "#     'ff_dim': [64, 128, 256],\n",
    "#     'num_blocks': [1, 2, 3],\n",
    "#     'l2_reg': [1e-3, 1e-4],\n",
    "#     'dropout_rate': [0.1, 0.3, 0.5]\n",
    "# }\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)  # 3-fold cross-validation\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and score\n",
    "# print(\"Best parameters:\", grid_result.best_params_)\n",
    "# print(\"Best validation accuracy:\", grid_result.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, Dropout, Input, GlobalAveragePooling1D\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define the Transformer-based model-building function\n",
    "# def build_model(trial, input_shape):\n",
    "#     # Suggest hyperparameters\n",
    "#     embed_dim = trial.suggest_int(\"embed_dim\", 32, 128, step=32)  # Embedding dimension\n",
    "#     num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2)      # Number of attention heads\n",
    "#     ff_dim = trial.suggest_int(\"ff_dim\", 64, 256, step=64)        # Feedforward network dimension\n",
    "#     num_blocks = trial.suggest_int(\"num_blocks\", 1, 3)            # Number of transformer blocks\n",
    "#     dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "#     l2_reg = trial.suggest_loguniform(\"l2_reg\", 1e-4, 1e-2)       # L2 regularization strength\n",
    "\n",
    "#     # Input layer\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     # Dense embedding layer\n",
    "#     x = Dense(embed_dim, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    \n",
    "#     # Add Transformer blocks\n",
    "#     for _ in range(num_blocks):\n",
    "#         x = TransformerBlock(embed_dim, num_heads, ff_dim, rate=dropout_rate, regularizer=l2(l2_reg))(x, training=True)\n",
    "    \n",
    "#     # Global pooling and output layer\n",
    "#     x = GlobalAveragePooling1D()(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     outputs = Dense(1, activation=\"sigmoid\", kernel_regularizer=l2(l2_reg))(x)\n",
    "    \n",
    "#     # Build and compile the model\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "#     model.compile(optimizer=Adam(learning_rate=trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)),\n",
    "#                   loss='binary_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the best hyperparameters\n",
    "# #best_params = study.best_params\n",
    "\n",
    "# best_params = {'epochs': 150, 'embed_dim': 128, 'num_heads': 8, 'ff_dim': 256, '\n",
    "# num_blocks': 1, 'dropout_rate': 0.35062671547806423, 'l2_reg': 0.00010345073760147071, 'learning_rate': 0.00024969942836710864}\n",
    "\n",
    "# # Build the final model using the best hyperparameters\n",
    "# final_model = build_model(\n",
    "#     trial=optuna.trial.FixedTrial(best_params),  # Use fixed trial with best params\n",
    "#     input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "# )\n",
    "\n",
    "# # Train the final model\n",
    "# final_model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=500,  # Use full training time\n",
    "#     batch_size=16,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate on the test set\n",
    "# test_loss, test_accuracy = final_model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM attempts - did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0011450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define function to normalize joint angles across all data\n",
    "# def normalize_data(data_list):\n",
    "#     \"\"\"\n",
    "#     Normalizes a list of numpy arrays using MinMax scaling across all features.\n",
    "#     Arguments:\n",
    "#         data_list: List of numpy arrays where each array corresponds to one time series.\n",
    "#     Returns:\n",
    "#         normalized_data: List of normalized numpy arrays.\n",
    "#     \"\"\"\n",
    "#     # Flatten all data for joint normalization across the dataset\n",
    "#     all_data = np.array(data_list)\n",
    "    \n",
    "#     # Apply MinMaxScaler\n",
    "#     scaler = MinMaxScaler()\n",
    "#     scaler.fit(all_data)  # Fit scaler on all joint angles across the dataset\n",
    "    \n",
    "#     # Normalize each time series\n",
    "#     normalized_data = [scaler.transform(data) for data in data_list]\n",
    "#     return normalized_data, scaler\n",
    "\n",
    "# # Define function to pad time series to the same length\n",
    "# def pad_time_series(data_list, max_length=None):\n",
    "#     \"\"\"\n",
    "#     Pads all time series in data_list to the same length.\n",
    "#     Arguments:\n",
    "#         data_list: List of numpy arrays where each array corresponds to one time series.\n",
    "#         max_length: If None, pad to the longest time series. Otherwise, pad to max_length.\n",
    "#     Returns:\n",
    "#         padded_data: Numpy array of shape (num_samples, max_length, num_features).\n",
    "#     \"\"\"\n",
    "#     if max_length is None:\n",
    "#         max_length = max(len(data) for data in data_list)  # Find the longest time series\n",
    "#     padded_data = pad_sequences(data_list, maxlen=max_length, padding=\"post\", dtype=\"float32\")\n",
    "#     return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5747de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_lstm_model_2(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Masking(mask_value=0.0, input_shape=input_shape),  # Masking for padded values\n",
    "#         LSTM(128, return_sequences=True),  # First LSTM layer with increased units\n",
    "#         BatchNormalization(),  # Batch normalization to stabilize training\n",
    "#         Dropout(0.2),  # Reduced dropout rate\n",
    "#         LSTM(64, return_sequences=False),  # Second LSTM layer\n",
    "#         BatchNormalization(),  # Batch normalization after second LSTM layer\n",
    "#         Dropout(0.2),\n",
    "#         Dense(32, activation='relu'),  # Increased fully connected layer size\n",
    "#         Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.0001),  # Reduced learning rate\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f19810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Ensure consistent shuffling for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# def manual_train_val_test_split(data, labels, train_ratio=0.8, val_ratio=0.1):\n",
    "#     \"\"\"\n",
    "#     Manually splits data and labels into training, validation, and testing sets with the given ratios.\n",
    "#     Arguments:\n",
    "#         data: Preprocessed time series data (numpy array).\n",
    "#         labels: Corresponding labels (numpy array).\n",
    "#         train_ratio: Proportion of data to use for training (default 0.7).\n",
    "#         val_ratio: Proportion of data to use for validation (default 0.2).\n",
    "#     Returns:\n",
    "#         X_train, X_val, X_test, y_train, y_val, y_test: Split data and labels.\n",
    "#     \"\"\"\n",
    "#     # Ensure train_ratio + val_ratio < 1\n",
    "#     assert train_ratio + val_ratio < 1, \"Train and validation ratios must sum to less than 1.\"\n",
    "    \n",
    "#     # Shuffle the indices\n",
    "#     indices = np.arange(len(data))\n",
    "#     np.random.shuffle(indices)\n",
    "    \n",
    "#     # Calculate the split points\n",
    "#     train_split = int(len(data) * train_ratio)\n",
    "#     val_split = int(len(data) * (train_ratio + val_ratio))\n",
    "    \n",
    "#     # Split the data and labels\n",
    "#     train_indices = indices[:train_split]\n",
    "#     val_indices = indices[train_split:val_split]\n",
    "#     test_indices = indices[val_split:]\n",
    "    \n",
    "#     X_train = data[train_indices]\n",
    "#     X_val = data[val_indices]\n",
    "#     X_test = data[test_indices]\n",
    "#     y_train = labels[train_indices]\n",
    "#     y_val = labels[val_indices]\n",
    "#     y_test = labels[test_indices]\n",
    "    \n",
    "#     return X_train, X_val, X_test, y_train, y_val, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34634a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "# def build_lstm_model_1(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Masking(mask_value=0.0, input_shape=input_shape),  # Masking for padded values\n",
    "#         LSTM(64, return_sequences=True),  # First LSTM layer\n",
    "#         Dropout(0.3),  # Dropout for regularization\n",
    "#         LSTM(32),  # Second LSTM layer\n",
    "#         Dropout(0.3),\n",
    "#         Dense(16, activation='relu'),  # Fully connected layer\n",
    "#         Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41a863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Perform the manual train-test split\n",
    "# # Call the function\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = manual_train_val_test_split(\n",
    "#     np.array(lstm_input), np.array(labels), train_ratio=0.8, val_ratio=0.1)\n",
    "# # Check the shapes of the resulting datasets\n",
    "# print(\"Training data shape:\", X_train.shape)\n",
    "# print(\"Training labels shape:\", y_train.shape)\n",
    "# print(\"Testing data shape:\", X_test.shape)\n",
    "# print(\"Testing labels shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "# # Convert to numpy arrays\n",
    "# X_train = np.array(X_train)\n",
    "# X_val = np.array(X_val)\n",
    "# X_test = np.array(X_test)\n",
    "# y_train = np.array(y_train)\n",
    "# y_val = np.array(y_val)\n",
    "# y_test = np.array(y_test)\n",
    "\n",
    "# # Define input shape (timepoints, features)\n",
    "# input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# # # Build model\n",
    "# # model = build_lstm_model(input_shape)\n",
    "\n",
    "# # # Train the model\n",
    "# # history = model.fit(\n",
    "# #     X_train, y_train,\n",
    "# #     validation_data=(X_val, y_val),\n",
    "# #     epochs=50,\n",
    "# #     batch_size=16,\n",
    "# #     verbose=1\n",
    "# # )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY 3 \n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Masking, LSTM, Dropout, Dense, BatchNormalization\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import numpy as np\n",
    "\n",
    "# # Define LSTM model with improvements\n",
    "# def build_lstm_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Masking(mask_value=0.0, input_shape=input_shape),  # Masking for padded values\n",
    "#         LSTM(128, return_sequences=True),  # Increased units in the first LSTM layer\n",
    "#         BatchNormalization(),  # Batch Normalization\n",
    "#         Dropout(0.4),  # Increased Dropout for regularization\n",
    "#         LSTM(64),  # Second LSTM layer with reduced units\n",
    "#         BatchNormalization(),  # Batch Normalization\n",
    "#         Dropout(0.4),\n",
    "#         Dense(32, activation='relu'),  # Increased units in fully connected layer\n",
    "#         Dropout(0.4),  # Additional dropout\n",
    "#         Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.0001),  # Reduced learning rate\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # Build the model\n",
    "# input_shape = (X_train.shape[1], X_train.shape[2])  # Define input shape based on your data\n",
    "# model = build_lstm_model(input_shape)\n",
    "\n",
    "# # Compute class weights for handling class imbalance\n",
    "# class_weights = compute_class_weight(\n",
    "#     'balanced',\n",
    "#     classes=np.unique(y_train),\n",
    "#     y=y_train\n",
    "# )\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# # Add EarlyStopping for regularization\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# # Train the model with validation and early stopping\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=50,\n",
    "#     batch_size=16,\n",
    "#     class_weight=class_weights,\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Evaluate the model on test data\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE LATEST LSTM\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "#from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Define the LSTM model\n",
    "# def build_lstm_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=input_shape),\n",
    "#         LSTM(256, return_sequences=True),  # Increase units\n",
    "#         Dense(128, activation='relu'),  # Add a dense layer\n",
    "#         LSTM(128, return_sequences=True),  # Second LSTM layer\n",
    "#         Dense(64, activation='relu'),  # Add another dense layer\n",
    "#         LSTM(64),  # Final LSTM layer\n",
    "#         Dense(32, activation='relu'),  # Fully connected layer\n",
    "#         Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),  # Higher LR\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Masking, LSTM, Dense, Dropout, BatchNormalization\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Build an LSTM model optimized for overfitting to the training set\n",
    "# def build_overfitting_lstm(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Masking(mask_value=0.0, input_shape=input_shape),  # Masking padded values\n",
    "#         LSTM(128, return_sequences=True, activation='relu'),  # First LSTM layer\n",
    "#         LSTM(64, return_sequences=True, activation='relu'),   # Second LSTM layer\n",
    "#         LSTM(32, activation='relu'),                         # Third LSTM layer\n",
    "#         Dense(1, activation='sigmoid')                       # Output layer for binary classification\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.001),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "# # Train the model on training data only\n",
    "# input_shape = (X_train.shape[1], X_train.shape[2])  # Define input shape from the training data\n",
    "# model = build_overfitting_lstm(input_shape)\n",
    "\n",
    "# # Train the model for an extended number of epochs\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=100,  # Increase epochs for training\n",
    "#     batch_size=16,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Plot training loss over epochs\n",
    "# plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5b52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# def build_lstm_model(input_shape):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=input_shape),\n",
    "#         LSTM(256, return_sequences=True),  # Larger capacity\n",
    "#         LSTM(128, return_sequences=True),\n",
    "#         LSTM(128, return_sequences=True),\n",
    "#         LSTM(64),\n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dense(1, activation='sigmoid')\n",
    "#     ])\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.0001),\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy']\n",
    "#     )\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Build model\n",
    "# input_shape = (X_train.shape[1], X_train.shape[2])  # Define input shape from the training data\n",
    "# model = build_lstm_model(input_shape)\n",
    "\n",
    "\n",
    "# # Train the model with class weights\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     epochs=500,\n",
    "#     batch_size=16,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Extract the loss values from the history object\n",
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# # Print loss values for each epoch\n",
    "# print(\"Training Loss per Epoch:\")\n",
    "# for epoch, loss in enumerate(train_loss):\n",
    "#     print(f\"Epoch {epoch+1}: Training Loss = {loss:.4f}, Validation Loss = {val_loss[epoch]:.4f}\")\n",
    "\n",
    "# # Plot the training and validation loss\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(train_loss, label='Training Loss', marker='o')\n",
    "# plt.plot(val_loss, label='Validation Loss', marker='o')\n",
    "# plt.title('Training and Validation Loss Over Epochs')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
